# 22/04/2024

## Reservoir Computing with Forward Anarchist Units



## Evo-topo (ft Naomi)

Nous n'avons pas beaucoup avancé ces dernières semaines, cependant nous avons eu quelques résultats assez prometteurs avec NEAT. 

Nous avons notamment observer qu'au bout de plusieurs dizaines de générations nous réussissions à obtenir des modèles obtenant plus de 60% de réussite au problème en utilisant seulement 9 units.  
Pour rappel, sans cette méthode, les meilleurs résultats que Naomi a obtenu jusqu'à présent sont de 80% pour 500 neurones, alors que l'aléatoire donnerait 25%.

Nous observons aussi que les meilleurs individus sont obtenus losque nous commençons avec des individus possèdant 1 seul neurone. Si nous commençons avec plus, l'algo génétique n'est plus a l'origine de chacun de ces neurones, et par conséquent certains d'entre eux ne sont vraiment pas optimaux et se maintiennent au fil des générations.

## ReservoirBERT

### Parallèlisation des calculs

Avec l'aide de Paul, j'ai pu modifier mon script d'entrainement pour utiliser les noeud `ESN` au lieu de composé moi même mon architecture via les noeud `Input`, `Reservoir` et `Ridge`. Ces noeud `ESN` permettent en utilisant le paramètre `workers=-1`de parallèliser les calculs sur l'ensemble des CPUs de l'ordinateur. 

De plus, nous avons changé la méthode d'entrainement. Au lieu d'entrainer mes ESN en online via RLS, nous faisons un entrainement offline, ESN après ESN, permettant un gain de temps **considérable**, tel que :

1. **12 x 1.000 units, 1.000 samples :**  l’entrainement prends environ 5 minutes, au lieu de 300 minutes précédemment. 

2. **12 x 10.000 units, 1.000 samples :** l’entrainement prends environ 80 minutes, au lieu de 20.000 minutes (estimées) précédemment.

*(Résultats obtenus son mon ordinateur.)*

On peut donc envisager des entrainements bien plus costaud (plus de données, plus de neurones). J'ai donc lancé via plafrim un entrainement sur 10.000 textes (soit environ 2.5M de tokens) et 12 ESN de 10.000 units. 

### Meilleur standardisation pour de meilleur résultats

En parralèle de ces optimisations, j'ai pu améliorer (corriger ?) ma standardisation pour ce problème. Ramenant toutes mes valeurs entre `[-1, 1]`, au lieu de l'intervalle `[-3, 3]`. Cet intervalle étant bien plus intéressant pour l'entrainement d'ESN.

Mes résultats se sont ainsi améliorer. Et prenne aussi bien plus de sens. 

Voici ce que l'on obtiens pour 12 ESN de 10.000 neurones s'entrainant sur 1.000 textes : 

```
MSE layer 0: 0.010947482806708543
MSE layer 1: 0.020600225123221582
MSE layer 2: 0.030639898507378624
MSE layer 3: 0.03744137388033629
MSE layer 4: 0.04147068722629457
MSE layer 5: 0.04657889986539417
MSE layer 6: 0.049359773494836924
MSE layer 7: 0.05179795107840946
MSE layer 8: 0.057281950497889096
MSE layer 9: 0.058559377366580574
MSE layer 10: 0.056101669395587056
MSE layer 11: 0.056837898071326845
```

*(Je me sens un peu naze de ne pas avoir vu / fait ça avant, mais mieux vaut tard que jamais...)*

L'une de mes prochaines étapes sera de relancer une recherche d'hyper-paramètres, car ceux utiliser actuellement sont toujours étranges, et ne doivent pas être optimaux pour la standardisation que j'utilise :

```
units = 10000
leak_rate = 0.985
spectral_radius = 2.8
input_scaling = 5.0
connectivity = 0.003
input_connectivity = 0.05
ridge = 0.276
seed = 42
```

## Deploiment LLM Inria (tests)

Nous avons continuer nos tests de perfomance de déploiment de LLM sur des A100s de Plafrim. Nous avons continuer avec Mixtral 8x7b, mais aussi avec Mixtral 8x22b - et notamment sa version finetune par Microsoft : WizardLM-2-8x22b - rendu publique il y a une dizaine de jour seulement. 

### Pour Mixtral 8x7B

![8x7-time](./picture-2024-04-22/8x7-time.png)
![8x7-token_per_second](./picture-2024-04-22/8x7-token_per_second.png)
![8x7-total_tokens](./picture-2024-04-22/8x7-total_tokens.png)


### Pour Mixtral 8x22B

![8x22b-time](./picture-2024-04-22/8x-22b-time.png)
![8x22b-token_per_second](./picture-2024-04-22/8x22b-tokens_per_second.png)
![8x22b-total_tokens](./picture-2024-04-22/8x22b-total_tokens.png)

En fin de semaine dernière, Meta a rendu publique de nouveaux modèles : Llama 3 (8b et 70b), apportant là aussi une révolution. On peut voir entre autre que la version 8B semble se positionner au dessus de Mixtral 8x7b.  

Bref, les modèles open-weights et les petits modèles semblent avoir un bel avenir devant eux. 

![Llama3-benchmarks](./picture-2024-04-22/llama3-benchmarks.png)

## Comité de Suivi Individuel

Nous avons composé notre Comité de Suivi Individuel. Ses membres sont les suivants :

- Nathanaël Fijalkow 
- Pierre-Yves Oudeyer

Notre première réunion est planifiée pour le 25 juin à 14h00. 

## Papers

- [Neural Turing Machine (2014)](https://arxiv.org/pdf/1410.5401.pdf)

The Neural Turing Machine (NTM) is a neural network architecture that includes an external memory component, allowing it to store and manipulate information in a way that is analogous to a traditional computer's memory. The NTM uses a differentiable attention mechanism to read from and write to this memory, allowing it to learn to perform complex tasks that require reasoning and memory. The NTM is trained using gradient descent, making it possible to learn its parameters from data.

- [The Forward-Forward Algorithm: Some Preliminary Investigations (2022)](https://arxiv.org/pdf/2212.13345.pdf)

The Forward-Forward Algorithm (FF) is a new learning procedure for neural networks that aims to replace the forward and backward passes of backpropagation with two forward passes, one with positive data and the other with negative data. Each layer in the network has its own objective function, which is to have high goodness for positive data and low goodness for negative data, and the sum of the squared activities in a layer can be used as the goodness. The FF algorithm is different from backpropagation in that it does not require the computation of gradients or the storage of intermediate values, and it can be used in situations where the precise details of the forward computation are unknown.


- [Predictive Coding (2011)](https://sci-hub.3800808.com/https://doi.org/10.1002/wcs.142)

Predictive coding is a framework that explains how the nervous system reduces redundancy and efficiently codes sensory information by transmitting only the unpredicted portions of an incoming signal. This framework provides a functional explanation for various neural responses and aspects of brain organization, such as center-surround response properties and biphasic temporal antagonism in the retina and lateral geniculate nucleus, as well as oriented receptive fields and contextual effects in the higher visual system.

- [Predictive Coding A Theoritical and Experimental Review (2022)](https://arxiv.org/pdf/2107.12979.pdf)

Predictive coding is a theory of brain function that suggests the brain's primary objective is to minimize prediction errors, which are the differences between predicted and actual sensory input. The theory proposes that the brain achieves this through a hierarchical network of layers, each making predictions about the activity of the layer below and passing prediction errors upwards to update higher-level predictions.

- [Representation Learning with Contrastive Predictive Coding (2018)](https://arxiv.org/pdf/1807.03748.pdf)

Contrastive Predictive Coding (CPC) is a method for unsupervised learning that combines autoregressive modeling and noise-contrastive estimation to learn abstract representations of high-dimensional data. CPC has been shown to achieve strong or state-of-the-art performance in a variety of domains, including audio, images, natural language, and reinforcement learning, when used as stand-alone features or in conjunction with the main loss.