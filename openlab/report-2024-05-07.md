# 07/05/2024

## Foward Anarchist Units

J'ai donc cherché à implémenter des unités comme décrite dans le schéma du dessus. Etant donné que chacune de ces matrices pouvait apprendre "indépendamment" de l'autre, j'ai choisi comme algorithme d'apprentissage deux simples regressions linéaires. Puis j'ai testé en créant des fonctions non linéaires "simples" (sinus, cosinus, quadratique) et d'autres fonctions plus complexes générées aléatoirement. 

***--> Nous ne convergeons pas systématiquement.***  

Et, en général, lorsque nous convergeons, une seule dimension est prédite plus ou moins correctement, les autres non. 

On peut noter les conclusions suivantes :
- L'entrainement indépendant et simultannée de plusieurs matrices qui agissent entre elles n'est pas simple. Il complique très fortement la convergence car chaque matrice apprends à recevoir des entrées qui ne sont plus correcte à l'étape d'optimisation suivante. 
- La somme que l'on calcule pour obtenir une seule sortie par unité n'est pas très performante ni maline car on ne l'entraine jamais : nos matrices ne reçoivent pas de retour des unités suivantes pour indiquer si la somme produite a de la valeur ou non.
- L'architecture est "trop compliqué" et hasardeuse car elle ne part pas de base que l'on connait très bien. Il serait plus malin de partir d'architecture déjà connue et matrisée. 

## Attention Unit Reservoir

### Attention block from Attention is All you Need

Suite à nos discussions et aux précédentes conclusions, j'ai modifié l'architecture que je mentionne plus haut. Au lieu de créer mes propres units, je prends comme units des blocks d'attention tels que définis dans le papier "Attention is all you need" (voir schéma plus bas). Seulement, j'y apporte trois petites spécificités : 
1. Ces blocs d'attention doivent travailler à un niveau très local, par conséquent j'envisage des inputs de très petite dimension (de l'ordre de 3-5). 
2. Chaque bloc d'attention doit apprendre via backpropagation à prédire ses inputs au temps `t+1`. La backpropagation se limite au bloc d'attention : les gradients se calcul au sein d'un block et ne sont pas altérés par les autres blocs d'attention.
3. Chaque unité contient une mémoire qui lui est propre est qui correspond à ses `M` dernières entrées. `M` étant un hyperparamètre du modèle (plus M est petit, plus la matrice d'attention calculées - qui grossit selon `M^2` - sera petite). 

![Attention Block](./picture-2024-05-07/Attention_bloc.png)

### Un Reservoir d'Unité d'Attention

Si l'on décide de relié aléatoirement ces différentes unités, on obtiens quelque chose qui ressemble à ça :

![Attention Reservoir](./picture-2024-05-07/attention_reservoir.png)

On peut noter les choses suivantes :
- Une unité calcule donc autant de sortie qu'elle prend d'entrée (car elles prédit ses entrées à `t+1`). Ceci nous évite de faire une somme non optimisée à la sortie de notre cellule.
- Si chaque unité possède autant de sortie que d'entrée, et qu'on doit en plus prendre en compte des connexions extérieurs (les inputs et feedback du graphe), alors nécessairement toutes les sorties ne pourront être connecté à une autre unité (comme visible sur le schéma). Cela dit, toutes les sorties sont utilisées pour la regression linéaire.  
- A l'instar de ce que l'on a rencontré précédemment, l'apprentissage simultanée de toutes les unités risque d'être compliqué et de ne pas converger car les entrées d'une unité dépendent des sorties d'autres unités. Par conséquent, si toutes les unités modifient leur poids simultanément, chaque unité apprends à reconnaitre des entrées qui ne seront plus valide au pas d'optimisation suivant. **Une solution peut être d'entrainer une seule unité à chaque batch, en sélectionnant celle ayant la plus grosse loss.** Voire, de sélectionner plusieurs unités qui sont suffisement éloignée les unes des autres : c'est à dire non connectées, ou connectées mais à une certaines distance minimum. 
- Actuellement, il n'existe qu'un seul type d'unité : les blocs d'attention. Cela dit, peut être qu'à l'avenir il pourrait être bon de décomposer nos blocs d'attention en deux différents blocs : un bloc mémoire (Feed Forward) et un bloc d'attention (Masked Multi-Head Attention).

Ainsi pour entrainer notre modèle, nous pouvons utiliser l'algo suivant :
```
1. Run un batch
2. Calculer la loss du batch pour chaque unité
3. Selectionner l'unité ayant la plus grande loss, et y appliquer la descente de gradient. 
4. Nettoyer le gradient des autres unités
5. Retour étape 1.
```

### Matrice de passage

### RL : Unité en Symbiose

## Evo-topo (feat Naomi)

Le projet avance lentement car nous ne nous voyons pas très fréquemment et nous avons connu une grosse période où nous n'arrivions pas à correctement exploiter nos résultats (mauvais enregistrements, quelques oublis et erreurs).  
Cela dit, nous observons des résultats qui semblent intéressants : 71% en score pour 14 neurones agencés via l'algo génétique, là où nous obtenons environ 65% avec 50 neurones agencés aléatoirement mais avec des hyper-paramètres pré-sélectionnés.  
Seulement, même si ces scores paraissent aléchants, nous devons vérifié qu'ils n'ont pas été obtenu sur un coup de chance. Nous devons entre autres les confirmer sur plusieurs seeds...

Aussi, j'ai commencé à composer quelques visualisations, et j'ai trouvé la visualisation (RSSViz) proposée par Alexandre Variengien et toi-même par très intéressante et facile à mettre en place. Elle plait aussi à Naomie, et nous pensons tous deux l'utiliser pour visualiser et comprendre l'activité de nos Reservoirs.

## Summerschool

J'ai candidaté aux deux SummerSchool suivantes : 

- [ILCB](https://www.ilcb.fr/2024-2/)
- [Pl@ntAgroEco](https://sites.google.com/view/agroeco-summer-school-2024/home)

## Papers

