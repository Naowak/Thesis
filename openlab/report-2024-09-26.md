# 26/09/2024

Je n'ai pas produit de résumé de mes travaux depuis environ 2 mois. Ceux-ci ont été pas mal entrecoupé par de nombreux problèmes, mais voici un compte rendu détaillé de mon avancement.

## Projet ReservoirChat et encadrement du stage de Virgile

Cet encadrement m'a pris vraiment beaucoup de temps. Il y a eu des semaines où j'ai passé la majorité de mon temps sur ce projet à aider Virgile. 
Il a fait un très bon travail, mais malheureusement était très loin d'être autonome sur ce projet ambitieux, ce qui est normal pour un stagiaire niveau M1. 
Cela dit, je trouve le résultat est vraiment très intéressant, et je suis heureux d'avoir pu plonger dans méthodes. 

## Cours

### IOGS

J'ai trois séances de TP à encadrer, la première a eu lieu cette semaine et les deux autres auront lieu ce mois-ci.

### ENSTBB

J'ai contacté Nobert Balakara (directeur de l'école) qui m'a mis en contact avec Joris Sansen qui est en charge de leur cours en informatique.
J'ai rencontré Joris et nous avons discuté sur le fait que j'y dispense un cours au second semestre, mais ce dernier devrait être ludique, pour leur donner envie de découvrir le monde de l'IA malgré leur niveau extrêmement basique.
Bien que nous avons annoncé des idées le cours n'est pas encore défini ni certains. 

### M1 INFO

Je vais donner des cours pour Akka Zemmari les lundi de 14 à 16h sur la période fin octobre - décembre. 

## SummerSchool ILCB

J'ai donc participé à la SummerSchool de l'ILCB sur le campus de Luminy à Marseille.  
J'ai eu au début quelques difficultés à trouver quels cours été adapté à mon niveau : les cours d'IA était clairement très bas niveau, et à l'inverse certains cours de neurosciences/langage était assez avancé. Nous étions très peu à provenir d'un cursus informatique/math, donc ceci est compréhensible. Il y avait aussi des moments où aucun cours ne correspondait à mes problématiques de recherche, mais c'était tout de même intéressant.  
J'ai aussi eu beaucoup l'impression que, bien que certains les méprisaient il y a peu, les Modèles de Langages venait de s'imposer dans leur domaine. Du coup, j'ai vu beaucoup de personne très épatées par la "magie" que ces modèles font.  
Mais, malgré tout ça, j'ai tout de même eu l'occasion de découvrir des choses intéressantes sur le cerveau, le comportement et le langage, et j'ai vraiment apprécié être plongé dans un tel environnement pendant une semaine.

## Réinscription Doctorat

J'ai débuté les démarches pour ma réinscription, j'attends la validation de l'Ecole Doctorale pour la finaliser.

## Papier : Deploying Open-Source Large Language Models: A performance Analysis

J'ai donc publié sur arXiv le papier des tests de performances des modèles de langages sur les machines de PlaFrim. 

Voici le lien : [Deploying Open-Source Large Language Models: A performance Analysis](https://arxiv.org/abs/2409.14887)


## Recurrent Attention Network

### Tests

J'ai donc pu faire des recherches d'HP axés sur le nombre d'unité et le nombre de dimension par unité sur deux datasets : CSL & Japanese Vowels (voir ci dessus). 
Il faudrait que je test bien d'autres paramètres, comme l'importance de la taille du contexte et les inputs duplications. 
Et surtout, que je tests sur d'autres tâches. 

Cela dit plafrim est de plus en plus surchargé, il devient assez compliqué d'obtenir une machine efficace, je ne sais pas où en sont les accès à Jean Zay mais ce serait très intéressant de les avoir prochainement.

### Améliorations

#### Parallelisation

J'ai finalement réussi cette semaine à rendre l'ensemble des calculs parallélisables : intra et inter units. Mais je n'ai pas encore eu le temps de mesurer les gains de performances, notamment que cela varie selon la machine. 
Pour cela je dois faire des layers de la forme (units, input_dim, output_dim) que je peux multiplié par des inputs de la forme (batch, units, context, input_dim) pour obtenir des outputs de la forme (batch, units, context, output_dim).  

Pour m'assurer de la validité, j'ai évidemment fait un ensemble de test avec les mêmes données et les mêmes poids j'obtiens exactement les mêmes résultats (gradient inclus). 

A noté que la parallèlisation que j'ai effectué sera aussi possible avec des unités différentes (mémoire, attention) selon certaines conditions (matrice de même dimensions, etc).
Cela dit, selon les choix que nous prenons, peut-être devront nous faire des calculs séparés pour chaque type d'unité.

#### Gestion de l'output

Pendant mes travaux de parallélisation, j'ai observé que j'ai conservé la façon dont les transformers calculs leur output, en séparant les dimensions du context. Donc, pour un context de 4, j'ai actuellement 4 sorties de dimension output_dim. 
Mais, ces 4 sorties se calculs sur des informations différentes produites par le modèle (hidden state). Or, je n'utilise qu'une seule de ces sorties, la dernière ! 
Et c'est stupide as duck !  
Je perds donc de l'information, et je pourrais potentiellement améliorer les performances en envoyant les 4 lignes d'informations produite par le modèle. 

#### Idées en vrac

- Empêcher qu'une entrée soit dupliqué deux fois dans la même unit 
- Poids des matrices d'attention aléatoire et fixe (Wq, Wk ?)
- Ajout d'un paramètre leak rate lors de l'ajout des résidus dans les unités
- Différents type d'unité, en commençant par coupé en deux les unités d'attention
- Utiliser une Porte permettant de déterminer s'il faut faire un tour interne de plus ou non pour produire l'output
- Utiliser une porte pour choisir sur quels unités redigirer les informations 


### Poster

Voici le poster que j'ai réalisé pour la SummerSchool de l'ILCB

[Version pdf](./picture-2024-09-26/final%20poster-a0.pdf)

Version PNG : 

![poster](./picture-2024-09-26/final%20poster-a0.png)
